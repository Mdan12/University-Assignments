{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/magnusde93/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/magnusde93/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 113917 tokens overall\n",
      "There are 20592 unique tokens\n",
      "10 of the most common tokens are [(',', 15085), ('a', 6355), ('.', 4731), ('az', 2719), ('hogy', 1867), ('nem', 1777), ('és', 1698), ('is', 1313), ('volt', 925), ('meg', 707)]\n",
      "10 of the most common tokens are [(',', 15085), ('a', 6882), ('.', 4731), ('az', 3004), ('nem', 1992), ('hogy', 1937), ('és', 1755), ('is', 1315), ('volt', 934), ('meg', 732)]\n",
      "15659 words are longer than 8 characters and they are {'INTÉZETBE', 'kisasszonynak', 'esztendőn', 'otthonában', 'szólított', 'mindentől', 'bekövetkezett', 'megfosztotta', 'pusztított', 'alkalmazott'} and more.\n",
      "The longest words are {'telefonbeszélgetésükkor'} and are 23 characters long\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"(\\b\\w+(?:'\\w+)?|[.,!?;:-])\"\n",
    "\n",
    "with open(\"abigel.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokenizer = re.findall(pattern, text)\n",
    "print(f\"There are {len(tokenizer)} tokens overall\")\n",
    "print(f\"There are {len(set(tokenizer))} unique tokens\")\n",
    "print(\"10 of the most common tokens are\", Counter(tokenizer).most_common(10))\n",
    "\n",
    "lower_tokenizer = re.findall(pattern, text.lower())\n",
    "print(\"10 of the most common tokens are\", Counter(lower_tokenizer).most_common(10))\n",
    "\n",
    "words_8 = [word for word in tokenizer if len(word) > 8]\n",
    "print(f\"{len(words_8)} words are longer than 8 characters and they are {set(words_8[:10])} and more.\")\n",
    "\n",
    "longestWordLength = len(max(tokenizer, key=len))\n",
    "print(f\"The longest words are {set([textword for textword in tokenizer if len(textword) == longestWordLength])} and are {longestWordLength} characters long\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 4251), ('PRP', 3783), ('.', 3492), ('IN', 2867), ('DT', 2854), ('VBD', 2702), ('RB', 2213), ('NNP', 2191), (\"''\", 1702), (',', 1679)]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "def get_pos_tag_frequencies(tagged_tokens, num_tags=10):\n",
    "    pos_tags = [pos for _, pos in tagged_tokens]\n",
    "    tag_freq = Counter(pos_tags)\n",
    "    return tag_freq.most_common(num_tags)\n",
    "\n",
    "top_pos_tags = get_pos_tag_frequencies(tagged_tokens)\n",
    "\n",
    "print(top_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: {'DT', 'NNP'}\n",
      "south: {'RB', 'NN'}\n",
      "drops: {'NNS', 'VBZ'}\n",
      "close: {'NN', 'VBZ', 'RB', 'JJ'}\n",
      "warm: {'VB', 'NN', 'JJ'}\n",
      "slipped: {'VBD', 'VBN'}\n",
      "over: {'RB', 'RP', 'NN', 'IN'}\n",
      "yellow: {'NN', 'JJ'}\n",
      "before: {'RB', 'IN'}\n",
      "one: {'NN', 'CD'}\n"
     ]
    }
   ],
   "source": [
    "def find_words_with_multiple_tags(tagged_tokens):\n",
    "    word_pos_mapping = defaultdict(set)\n",
    "\n",
    "    for word, pos in tagged_tokens:\n",
    "        word_pos_mapping[word].add(pos)\n",
    "\n",
    "    ambiguous_words = {word: tags for word, tags in word_pos_mapping.items() if len(tags) > 1}\n",
    "    return ambiguous_words\n",
    "\n",
    "\n",
    "ambiguous_words = find_words_with_multiple_tags(tagged_tokens)\n",
    "ambiguous_items = list(ambiguous_words.items())[:10]\n",
    "\n",
    "for word, tags in ambiguous_items:\n",
    "    print(f\"{word}: {tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['’'] 15\n"
     ]
    }
   ],
   "source": [
    "def word_with_most_tags(ambiguous_words):\n",
    "    if not ambiguous_words:\n",
    "        return None\n",
    "\n",
    "    max_tags = max(len(tags) for tags in ambiguous_words.values())\n",
    "    words_with_max_tags = [word for word, tags in ambiguous_words.items() if len(tags) == max_tags]\n",
    "\n",
    "    return words_with_max_tags, max_tags\n",
    "\n",
    "words_with_max_tags, max_tags = word_with_most_tags(ambiguous_words)\n",
    "\n",
    "print(words_with_max_tags, max_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('.', '.'), 3123), ((',', ','), 1679), ((\"''\", \"''\"), 1269), (('``', '``'), 1237), (('the', 'DT'), 1190), (('and', 'CC'), 777), (('I', 'PRP'), 741), (('a', 'DT'), 674), (('to', 'TO'), 578), (('you', 'PRP'), 508)]\n"
     ]
    }
   ],
   "source": [
    "def most_common_word_pos_pairs(tagged_tokens, num_pairs=10):\n",
    "    word_pos_pairs = [(word, pos) for word, pos in tagged_tokens]\n",
    "    word_pos_freq = Counter(word_pos_pairs)\n",
    "    return word_pos_freq.most_common(num_pairs)\n",
    "\n",
    "top_word_pos_pairs = most_common_word_pos_pairs(tagged_tokens, num_pairs=10)\n",
    "\n",
    "print(top_word_pos_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DT', 'JJ', 'NNP'), ('JJ', 'NNP', 'RB'), ('NNP', 'RB', 'IN'), ('RB', 'IN', 'NNP'), ('IN', 'NNP', ','), ('NNP', ',', 'DT'), (',', 'DT', 'NNP'), ('DT', 'NNP', 'NNP'), ('NNP', 'NNP', 'VBZ'), ('NNP', 'VBZ', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "def get_pos_tag_trigrams(tagged_tokens, num_trigrams=10):\n",
    "    pos_tags = [pos for _, pos in tagged_tokens]\n",
    "    pos_trigrams = [(pos_tags[i], pos_tags[i+1], pos_tags[i+2]) for i in range(len(pos_tags) - 2)]\n",
    "    return pos_trigrams[:num_trigrams]\n",
    "\n",
    "top_pos_tag_trigrams = get_pos_tag_trigrams(tagged_tokens, num_trigrams=10)\n",
    "\n",
    "print(top_pos_tag_trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('complex', 'JJ'), ('houses', 'NNS'), ('married', 'VBD'), ('and', 'CC'), ('single', 'JJ'), ('soldiers', 'NNS'), ('and', 'CC'), ('their', 'PRP$'), ('families', 'NNS'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This may cause'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The complex houses married and single soldiers and their families.\"\n",
    "\n",
    "tagged_sentence = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "print(tagged_sentence)\n",
    "\n",
    "\n",
    "'''I tried to find a sentence that would mess with the sentence and interpret the words in a wrong way but it seems as if NLTK is very good at interpreting the words in the right way.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "Manually gathering data.\n",
    "Pros: Extremely precise. Since it's a human gathering the data they can be very anal about the data.\n",
    "Cons: It's very expensive and it takes a long time.\n",
    "\n",
    "Scraping the internet.\n",
    "Pros: A lot of data available. Very cheap and so many different topics to choose from.\n",
    "Cons: The internet is full of useless data. Some websites have rules so you can't scrape them.\n",
    "\n",
    "Using crowd-sourcing.\n",
    "Pros: Large data available and not expensive. Can also help to have contributors of all backgrounds.\n",
    "Cons: Hard to control the quality especially when contributors have different skill levels.\n",
    "\n",
    "Using NLP systems.\n",
    "Pros: It's very fast and consistent across large datasets. There are pre-trained models out there to use.\n",
    "Cons: They aren't flawless and you can only use pre-trained models.\n",
    "\n",
    "It all depends on what we're working with. If I have enough money I would obviously manually gather the data, unless there's a website I can scrape from that's extremely reliable, like visindavefurinn.\n",
    "If I don't have money nor time I would scrape the internet, anything I could find that might be handy.\n",
    "\n",
    "2.\n",
    "It's important to anonymize data but some types of NLP systems require access to non-anonymized data like language models for content generation. GPT-3 needs to access diverse rext to generate human-like behaviour. Chatbots need data to provide information to clients. Speech recognition systems rely on human voices to convert it into text.\n",
    "\n",
    "3.\n",
    "If the AI is trained with prejudice it can have negative implications of these models. It can be racist, discriminate or use offensive language. If it's trained on prejudice it can base it's answer on the human it's trying to help. If it's racist it could give a bad answer to someone who's of a minority while giving others the answer needed. Gender, race, sexuality and more issues come up when the AI model is trained on prejudice. You're less likely to trust an AI model if it's prejudice.\n",
    "Great examlpe of a AI model is the Tay chatbot that was trained by human interaction. It became so offensive and racist that Microsoft had to take down the bot and issue an apology for the bot.\n",
    "\n",
    "4.\n",
    "I did run into some trouble with the regex. Decided to just put \"wasn't\" and words with \"n't\" as a single token. That fixed one problem. I wish the dots and commas weren't a part of the token even though I know they serve a big part of it all. It did take me time to google all of the tags for every word in NLTK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
